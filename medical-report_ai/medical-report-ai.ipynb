{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Brain Tumor AI Reporting System in Google Colab\n",
    "\n",
    "This notebook implements the two-phase medical reporting system.\n",
    "1.  **Phase 1:** A specialist model (`google/med-gemma`) analyzes raw medical data to produce a technical JSON output.\n",
    "2.  **Phase 2:** An orchestrator model (`llama3-70b-8192` via Groq API) translates the technical data into a user-friendly report and then enters a conversational Q&A loop, using a RAG system powered by a Pinecone vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU transformers accelerate bitsandbytes torch\n",
    "!pip install -qU langchain langchain-groq langchain_community python-dotenv pinecone-client sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set Up API Keys and Hugging Face Token\n",
    "\n",
    "To run this notebook, you need three secret keys:\n",
    "1.  `GROQ_API_KEY`: From [GroqCloud](https://console.groq.com/keys)\n",
    "2.  `PINECONE_API_KEY`: From [Pinecone](https://www.pinecone.io/)\n",
    "3.  `HF_TOKEN`: A Hugging Face token with access to Gemma models. Get one from [Hugging Face](https://huggingface.co/settings/tokens).\n",
    "\n",
    "**Instructions:**\n",
    "1. Click the 'ðŸ”‘' (key) icon in the left sidebar of Colab.\n",
    "2. Click '+ Add new secret' for each of the three keys listed above.\n",
    "3. Enable the 'Notebook access' toggle for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set environment variables from Colab secrets\n",
    "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
    "os.environ[\"PINECONE_API_KEY\"] = userdata.get('PINECONE_API_KEY')\n",
    "\n",
    "# Login to Hugging Face CLI to access Gemma models\n",
    "from huggingface_hub import login\n",
    "login(token=userdata.get('HF_TOKEN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Imports and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Pinecone as LangchainPinecone\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Set Up Pinecone Vector Database (RAG System)\n",
    "\n",
    "This cell will create a cloud-based vector index in your Pinecone account and populate it with simulated PubMed abstracts about brain tumors. This only needs to be run once. After the first successful execution, you can comment out the `setup_pinecone_index()` call at the end of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_INDEX_NAME = \"brain-tumor-pubmed\"\n",
    "\n",
    "# Sample documents simulating PubMed abstracts\n",
    "SAMPLE_DOCUMENTS = [\n",
    "    \"High-grade gliomas, such as glioblastoma, are highly aggressive brain tumors. On contrast-enhanced MRI, they classically present as irregularly shaped, ring-enhancing lesions due to a necrotic core and a hypervascular, leaky rim. The surrounding vasogenic edema is a result of the disruption of the blood-brain barrier by tumor-secreted factors.\",\n",
    "    \"The standard of care for a suspected high-grade glioma, known as the Stupp protocol, often involves maximal safe surgical resection, followed by concurrent radiation therapy and chemotherapy with the alkylating agent temozolomide (TMZ).\",\n",
    "    \"A differential diagnosis for a ring-enhancing lesion in the brain is critical. It primarily includes high-grade glioma (glioblastoma), brain metastasis from a primary cancer elsewhere (e.g., lung, breast, melanoma), and a pyogenic brain abscess. Advanced imaging techniques like MR Spectroscopy can help differentiate these.\",\n",
    "    \"Mass effect is a crucial radiological sign, referring to the secondary effects of a large lesion, such as the displacement of normal brain structures. A midline shift greater than 5mm is often associated with increased intracranial pressure and can be a neurosurgical emergency.\",\n",
    "    \"Stereotactic biopsy is a minimally invasive neurosurgical procedure used to obtain a tissue sample from a brain lesion for definitive histopathological diagnosis. This is often performed when a lesion is in an eloquent or deep-seated area where a full resection carries high risk.\",\n",
    "    \"Magnetic Resonance Spectroscopy (MRS) is a non-invasive imaging technique that analyzes the chemical composition of brain tissue. In brain tumors, an elevated choline peak and a reduced N-acetylaspartate (NAA) peak are indicative of high cellular turnover and neuronal loss, respectively, suggesting malignancy.\"\n",
    "]\n",
    "\n",
    "def get_embeddings_model():\n",
    "    return HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_pinecone_client():\n",
    "    return Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "def setup_pinecone_index():\n",
    "    pc = get_pinecone_client()\n",
    "    embeddings = get_embeddings_model()\n",
    "    embedding_dimension = embeddings.client[1].word_embedding_dimension\n",
    "\n",
    "    if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
    "        print(f\"Creating Pinecone index: {PINECONE_INDEX_NAME}...\")\n",
    "        pc.create_index(\n",
    "            name=PINECONE_INDEX_NAME,\n",
    "            dimension=embedding_dimension,\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "        )\n",
    "        print(\"Index created. Upserting documents...\")\n",
    "        LangchainPinecone.from_texts(texts=SAMPLE_DOCUMENTS, embedding=embeddings, index_name=PINECONE_INDEX_NAME)\n",
    "        print(\"Documents upserted successfully.\")\n",
    "    else:\n",
    "        print(f\"Index '{PINECONE_INDEX_NAME}' already exists.\")\n",
    "\n",
    "def get_retriever():\n",
    "    embeddings = get_embeddings_model()\n",
    "    vectorstore = LangchainPinecone.from_existing_index(index_name=PINECONE_INDEX_NAME, embedding=embeddings)\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# --- Run the one-time setup ---\n",
    "setup_pinecone_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Load Specialist LLM (Med-Gemma)\n",
    "\n",
    "Here, we download the `google/med-gemma` model from Hugging Face. We use 4-bit quantization (`BitsAndBytesConfig`) to ensure the model fits into the memory of a free Colab T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/med-gemma\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Create a LangChain-compatible LLM from the local model\n",
    "med_gemma_pipeline = torch.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    do_sample=True,\n",
    "    temperature=0.1, \n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "specialist_llm = HuggingFacePipeline(pipeline=med_gemma_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Define LLMs and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orchestrator LLM (via API)\n",
    "orchestrator_llm = ChatGroq(\n",
    "    temperature=0.4,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    max_tokens=2048,\n",
    ")\n",
    "\n",
    "# --- PROMPTS ---\n",
    "TECHNICAL_PROMPT_TEMPLATE = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a neuroradiology expert AI. Your task is to analyze the provided Brain MRI findings and convert them into a structured, raw JSON object. Focus exclusively on technical accuracy. Do not add conversational text or explanations. Your entire output must be only the JSON object.\n",
    "\n",
    "    MRI Data:\n",
    "    {medical_data}\n",
    "\n",
    "    Required JSON Output Format:\n",
    "    {{\n",
    "      \"summary\": \"One-sentence technical summary of the primary finding.\",\n",
    "      \"detailed_analysis\": {{\n",
    "        \"lesion_type\": \"Categorize the lesion based on its features (e.g., 'Ring-enhancing intra-axial mass').\",\n",
    "        \"location_specifics\": \"Provide a detailed anatomical location.\",\n",
    "        \"key_features\": [\"List key radiological features (e.g., 'Central necrosis', 'Significant vasogenic edema', 'Irregular enhancement').\"],\n",
    "        \"mass_effect_details\": \"Describe the mass effect observed (e.g., 'Sulcal effacement with 4mm rightward midline shift').\"\n",
    "      }},\n",
    "      \"differential_diagnosis\": [\"List the most likely differential diagnoses in order of probability (e.g., 'High-grade glioma (e.g., glioblastoma)', 'Metastasis', 'Abscess').\"],\n",
    "      \"recommendations\": [\"List technical next steps (e.g., 'Neurosurgical consultation for resection or biopsy', 'Advanced imaging such as MRS or Perfusion MRI').\"]\n",
    "    }}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "REPORT_GENERATION_TEMPLATE = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a compassionate medical communicator specializing in neurology. Your task is to translate a raw, technical neuroradiology analysis into a clear, well-structured report for a {target_audience}. Do not add any medical information not present in the technical analysis. Use the original patient data for context. Generate the final report using Markdown formatting.\n",
    "\n",
    "    **Raw Technical Analysis:**\n",
    "    {raw_analysis}\n",
    "\n",
    "    **Original Patient Data:**\n",
    "    {medical_data}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "QNA_RAG_TEMPLATE = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an AI assistant helping a user understand a brain tumor medical report. Use the provided chat history and the retrieved context from medical literature to answer the user's question. Frame your answer based on the retrieved context. If the context is relevant, relate it back to the specifics of the patient's case. If the context does not help, say that you cannot find information on that topic.\n",
    "\n",
    "    **Chat History:**\n",
    "    {chat_history}\n",
    "\n",
    "    **Retrieved Context from Medical Literature:**\n",
    "    {context}\n",
    "\n",
    "    **User Question:**\n",
    "    {question}\n",
    "\n",
    "    **Answer:**\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"LLMs and Prompts are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Define Input Data and Run Phase 1 (Report Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_data = {\n",
    "  \"patient_id\": \"P-78901\",\n",
    "  \"imaging_type\": \"Brain MRI with Contrast\",\n",
    "  \"findings\": [\n",
    "    {\n",
    "      \"finding_type\": \"Mass\",\n",
    "      \"location\": \"Left Frontal Lobe\",\n",
    "      \"size_mm\": [35, 30, 42],\n",
    "      \"characteristics\": \"Irregularly shaped, ring-enhancing lesion with central necrosis.\",\n",
    "      \"associated_edema\": \"Significant vasogenic edema in the surrounding white matter.\",\n",
    "      \"mass_effect\": \"Effacement of the adjacent cortical sulci and mild midline shift of 4mm to the right.\",\n",
    "      \"confidence_score\": 0.95\n",
    "    }\n",
    "  ],\n",
    "  \"image_metadata\": {\n",
    "    \"sequence\": \"T1-weighted post-contrast\",\n",
    "    \"slice_thickness_mm\": 5\n",
    "  }\n",
    "}\n",
    "\n",
    "def run_phase_1_report_generation(medical_data: dict):\n",
    "    print(\"--- ðŸ§  Phase 1: Initial Report Generation ---\")\n",
    "    medical_data_str = json.dumps(medical_data, indent=2)\n",
    "\n",
    "    specialist_chain = TECHNICAL_PROMPT_TEMPLATE | specialist_llm | JsonOutputParser()\n",
    "    print(\"> Invoking Med-Gemma specialist model...\")\n",
    "    raw_analysis = specialist_chain.invoke({\"medical_data\": medical_data_str})\n",
    "    print(\"< Specialist analysis received.\")\n",
    "\n",
    "    report_generation_chain = REPORT_GENERATION_TEMPLATE | orchestrator_llm | StrOutputParser()\n",
    "    print(\"> Invoking Llama-3 orchestrator model for report generation...\")\n",
    "    final_report = report_generation_chain.invoke({\n",
    "        \"target_audience\": \"patient\",\n",
    "        \"raw_analysis\": json.dumps(raw_analysis, indent=2),\n",
    "        \"medical_data\": medical_data_str\n",
    "    })\n",
    "    print(\"< Final report generated.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"        Final Medical Report\")\n",
    "    print(\"=\"*60)\n",
    "    print(final_report)\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    return final_report\n",
    "\n",
    "# Execute Phase 1\n",
    "final_report = run_phase_1_report_generation(medical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Run Phase 2 (Conversational Q&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_history(chat_history):\n",
    "    if not chat_history:\n",
    "        return \"No history yet.\"\n",
    "    return \"\\n\".join([f\"{msg.type.capitalize()}: {msg.content}\" for msg in chat_history])\n",
    "\n",
    "def run_phase_2_conversational_qna(initial_report: str):\n",
    "    print(\"--- ðŸ’¬ Phase 2: Conversational Q&A ---\")\n",
    "    print(\"The system is now ready for your follow-up questions.\")\n",
    "    print(\"Type 'exit' to end the session.\\n\")\n",
    "\n",
    "    retriever = get_retriever()\n",
    "    memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "    memory.save_context(\n",
    "        {\"input\": \"Here is the initial report for context.\"},\n",
    "        {\"output\": initial_report}\n",
    "    )\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"chat_history\": RunnableLambda(lambda x: format_chat_history(memory.load_memory_variables({})[\"chat_history\"]))\n",
    "        }\n",
    "        | QNA_RAG_TEMPLATE\n",
    "        | orchestrator_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"Your Question: \")\n",
    "            if question.lower().strip() == 'exit':\n",
    "                print(\"\\nSession ended. Goodbye! ðŸ‘‹\")\n",
    "                break\n",
    "\n",
    "            print(\"\\n> Searching knowledge base and generating response...\")\n",
    "            answer = rag_chain.invoke(question)\n",
    "            print(f\"\\nAssistant: {answer}\\n\")\n",
    "            memory.save_context({\"input\": question}, {\"output\": answer})\n",
    "\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nSession ended by user. Goodbye! ðŸ‘‹\")\n",
    "            break\n",
    "\n",
    "# Execute Phase 2 if Phase 1 was successful\n",
    "if final_report:\n",
    "    run_phase_2_conversational_qna(final_report)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}